% !TeX root = main.tex
\documentclass[a4paper,num-refs]{scr-contemporary}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\captionsetup{justification=centering}
\let\mathscr\undefined

\usepackage{newpxmath}
\usepackage{ragged2e}
\usepackage{booktabs}

% Surrey Capital Research Specifics
\jname{Surrey Capital Research} % Set the journal name to Surrey Capital Research

\secondlogo{../images/scr-logo.pdf} % Surrey Capital Research logo
\jlogo{../images/surrey-logo.pdf} % University of Surrey logo


\title{A Deep Dive into Portfolio Optimisation}

\author{\fontsize{12pt}{12pt}\selectfont Riccardo di Silvio, Jack Humphries, Sofiya Kolokolnikova, Freya Cullen, Alex Inskip}

\affil{Surrey Capital Research, University of Surrey}
\papercat{Quantitative Research}

\runningauthor{Surrey Capital Research}

\jvolume{01}
\jnumber{01}
\jyear{2026}

\numberwithin{equation}{section}

\begin{document}

\begin{frontmatter}
\maketitle

\begin{abstract}
  \justifying
\textbf{Background}:
  Selecting an optimal portfolio allocation across a universe of assets is a central problem in 
  investment management. While mean-variance optimisation, first proposed by Markowitz (1952), provides a 
  theoretically principled framework, its out-of-sample performance has proven disappointing in practice. 
  This study empirically compares four portfolio strategies to assess whether model sophistication translates 
  into superior investment performance. \\
\vspace{0.5mm}
\textbf{Methodology}:
  This study implements and compares four portfolio strategies: equal weight, Markowitz mean-variance, 
  Black-Litterman and Risk Parity applied to an 18-asset UK multi-asset portfolio comprising 15 FTSE 100 equities, 
  a UK government bond ETF, a gold ETC and a broad commodities ETF, over the period January 2015 to December 2025. 
  Strategies are evaluated using a monthly rebalancing framework and assessed across annualised return, volatility, 
  Sharpe ratio, maximum drawdown and portfolio turnover. \\
\vspace{0.5mm}
  \textbf{Results}:
  We find results broadly consistent with DeMiguel et al. (2009), with the naive benchmark 
  proving difficult to displace on a risk-adjusted basis over the full sample period. The optimisation 
  strategies exhibit meaningfully different risk profiles, however, suggesting that Sharpe ratio alone does not 
  fully capture the practical trade-offs between approaches. \\
\end{abstract}

\end{frontmatter}

\begin{keypoints*}
\begin{itemize}
  \item The equal-weight portfolio is robust because it does not rely on 
  estimated parameters and is therefore unaffected by estimation error.
  \item The Efficient Frontier is the set of portfolios that deliver the minimum 
  possible variance for each level of expected return.
  \item Empirical evidence suggests that many optimisation models fail to 
  consistently outperform the equal-weight portfolio out of sample.
\end{itemize}
\end{keypoints*}

\section{Introduction}
\subsection{The Portfolio Allocation Problem}
\indent \indent The portfolio allocation problem is the fundamental question in investment management. 
It asks: How should capital be distributed across available investment opportunities to best achieve 
an investor's objectives? \\
\indent The problem was intractable until 1952, when Harry Markowitz published \textit{Portfolio Selection} in the 
\textit{Journal of Finance} and turned it into a clean optimisation problem, and the “efficient frontier” 
gave investors a principled answer. Markowitz's mean variance model minimises portfolio variance for a given 
level of expected return. It was revolutionary in the consideration of variance as the risk, standing as a 
hallmark of modern portfolio theory. In practice however, the model has proven difficult to implement reliably. \\
\indent Michaud (1989) challenged this view directly, arguing that small estimation errors in expected 
returns produce wildly unstable portfolio recommendations. \\
\indent In 2009, DeMiguel et al. delivered a provocative result: he found that even sophisticated strategies 
incorporating shrinkage estimators and Bayesian methods failed to consistently outperform 1/N out-of-sample, 
suggesting that estimation error dominates theoretical optimality for typical portfolio problems. This tension 
between the theoretical elegance of optimisation and its empirical fragility motivates the present study. 


\subsection{Research Objectives}
\indent \indent This study addresses three main questions:
\begin{enumerate}
    \item Do optimisation models outperform a naive 1/N benchmark
    out-of-sample, applied to a multi-asset UK portfolio over 
    a ten-year period? 
    \item Which model offers the best risk-adjusted performance, 
    as measured by the Sharpe ratio?
    \item How does model performance vary across distinct market
    regimes: the Brexit referendum (2016), the COVID-19 crash (2020),
    and the 2022 rate-hiking cycle?
\end{enumerate}

\subsection{Structure of the Report}
\indent \indent The remainder of this report is organised as follows: \newline
Section 2 reviews the theoretical foundations of each model 
and derives their key mathematical results. Section 3 
describes the data, implementation choices, and backtesting 
framework. Section 4 presents the empirical results, including 
full-period performance metrics and a breakdown by market 
regime. Section 5 discusses the findings and their practical 
implications. Section 6 concludes.

\section{Literature and Theoretical Background}

This section details the design of your study, the data used, the models applied, and the analytical framework. 
The goal is to provide enough detail for another researcher to replicate your work.

\subsection{Notation}
Describe the data you used, where you got it from (e.g., Bloomberg, Refinitiv, web scraping), and any steps you 
took to clean, process, or transform it. Mention the time period and frequency of the data.

\subsection{The Equal Weight Puzzle}

\indent \indent The equal-weight (1/N) portfolio assigns an identical weight of 1/N to each asset in the portfolio, holding N assets, regardless of any asset-specific 
characteristics such as return, volatility, or correlation. 
\citet{demiguel_optimal_2009} tested 14 optimisation models across 7 datasets. None consistently outperformed the naive equal-weight portfolio: 
\begin{equation}
  w_i = \frac{1}{N} \quad \text{for} \quad 1 \leq i \leq N
\end{equation}
\indent But why? When expected returns are estimated from historical data in order to determine optimal portfolio weights, the estimates 
improve as more data is collected, but they do so slowly. \newline
Formally, the estimation error decays as
\begin{equation}
  \hat{\mu} - \mu = \mathcal{O}\!\left(\frac{1}{\sqrt{T}}\right)
\end{equation}
where $T$ is the number of observations.

Because the error shrinks at rate $\frac{1}{\sqrt{T}}$, halving the 
error requires four times as much data, meaning that with monthly data 
and realistic return distributions, approximately 500 years of observations 
are required for statistically reliable estimates, especially since these
errors are amplified through the term $\Sigma^{-1}\hat{\mu}$. \newline
\indent The equal-weight portfolio sidesteps this problem entirely: by assigning a fixed weight to each asset, it requires no parameter estimation 
and is therefore immune to estimation error by construction. \newline
The cost of this simplicity, however, is that the portfolio ignores all information about assets' risk and return characteristics, makes 
no attempt to diversify risk efficiently and cannot adapt to changing market conditions.


% ─────────────────────────────────────────────
\subsection{Mean-Variance Optimisation}
\label{sec:mvo}

\indent \indent \citet{markowitz_portfolio_1952} formulated \textit{Portfolio Selection} in 1952 as a constrained optimisation problem: minimise 
portfolio variance for a given level of expected return. His work stands as a hallmark of modern portfolio theory.\newline
\indent Formally, the problem states:
\begin{equation}                                         
  \begin{aligned}
  \min_{\mathbf{w}} \hat{\sigma} =
  \mathbf{w}^\top \hat{\boldsymbol{\Sigma}}\ \mathbf{w}
  \quad \text{s.t.} \quad
  \mathbf{w}^\top \boldsymbol{\mu} = \mu_p
  \quad \text{;} \quad 
  \mathbf{w}^\top\mathbf{1} = 1
  \end{aligned}
\end{equation}

with
  $\quad \quad \mathbf{w}, \boldsymbol{\mu} \in \mathbb{R}^N 
  \quad \text{and} \quad
  \hat{\boldsymbol{\Sigma}} \in \mathbb{R}^{N \times N} \quad \quad$ 
as the vector of portfolio weights, the vector of expected returns with
$\mu_p$ as the scalar target portfolio return and the sample covariance 
matrix of asset returns estimated from historical data respectively. \\

The first constraint, also known as the return constraint, fixes the portfolio to a 
specific point at the target level $\mu_p$, pinning the solution to a specific point on the "Efficient Frontier".
By varying $\mu_p$ across all feasible values, the complete set of minimum-variance 
portfolios - the Efficient Frontier - is traced out.
The second constraint, i.e. the budget constraint, ensures the weights represent a valid
allocation of wealth, hence that the sum of all weights is always equal to $1$. \newline
\indent It is important to note that no domain-restriction condition is imposed on the budget constraint,
each $w_i$ may take any real value, in particular, negative ones.
Practically speaking, a negative weight corresponds to a short position in the associated asset.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/plots/theory/efficient_frontier.pdf}
    \caption{Efficient Frontier estimated using sample covariance}
    \label{fig:efficient_frontier}
\end{figure}

Because the objective function is quadratic and the constraints are linear equalities, we cannot solve the problem by direct substitution 
(two constraints, n unknowns). We therefore use the method of Lagrange multipliers. 
The Lagrangian formulation is:

\begin{equation}
  \mathcal{L}(\mathbf{w}, \lambda, \gamma)
      = \mathbf{w}^\top \boldsymbol{\Sigma} \mathbf{w}
      - \lambda\!\left(\mathbf{w}^\top \boldsymbol{\mu} - \mu_p\right)
      - \gamma\!\left(\mathbf{1}^\top \mathbf{w} - 1\right)
\end{equation}

where $\lambda$ and $\gamma$ are the Lagrange multipliers associated with the return and budget constraints, respectively. 
Intuitively, $\lambda$ captures the marginal cost of requiring a higher return (i.e. how much additional variance must be accepted 
per unit increase in $\mu_p$), and $\gamma$ enforces full investment.

Taking the first-order condition:
\begin{equation}
  \frac{\partial \mathcal{L}}{\partial \mathbf{w}} = 2\boldsymbol{\Sigma} \mathbf{w} - \lambda \boldsymbol{\mu} - \gamma \mathbf{1} = \mathbf{0}
\end{equation}

Solving gives:
\begin{equation}
  \mathbf{w}_p = \frac{\lambda}{2}\,\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu} + \frac{\gamma}{2}\,\boldsymbol{\Sigma}^{-1}\mathbf{1}
\end{equation}

Imposing the budget and return constraints leads to the following two-fund theorem: all efficient portfolios can be expressed as a 
linear combination of any two distinct efficient portfolios.
\begin{equation}
  \mathbf{w}_A = \frac{\boldsymbol{\Sigma}^{-1}\mathbf{1}}{\mathbf{1}^\top \boldsymbol{\Sigma}^{-1}\mathbf{1}}
  \qquad
  \mathbf{w}_B = \frac{\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}}{\mathbf{1}^\top \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}}
\end{equation}

Note that $\mathbf{w}_A$ is the \emph{global minimum variance} (GMV) portfolio, 
the leftmost point on the efficient frontier, obtained by dropping the 
return constraint and minimising variance subject only to full investment:
\begin{equation}
  \mathbf{w}_{\mathrm{GMV}}
    = \frac{\boldsymbol{\Sigma}^{-1}\mathbf{1}}{\mathbf{1}^\top \boldsymbol{\Sigma}^{-1}\mathbf{1}}
\end{equation}

And the \emph{maximum Sharpe ratio} portfolio is:
\begin{equation}
  \mathbf{w}_{\mathrm{MSR}}
    = \frac{\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu} - r_f \mathbf{1})}
           {\mathbf{1}^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu} - r_f \mathbf{1})}
  \label{eq:msr}
\end{equation}
\indent Despite its theoretical elegance, Markowitz optimisation suffers from a well-documented problem. 
As mentioned above, the solution depends critically on $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$, both estimated from historical data. \newline
\indent Small errors in $\mathbf{\mu}$ can cause large shifts in optimal weights.
\citet{michaud_markowitz_1989} characterised the optimiser as an "error maximiser": it systematically overweights assets with 
overestimated returns and underestimated variances.

\begin{keypoints*}
\begin{itemize}
  \item Black-Litterman uses market-cap weights as a neutral 
  starting point for expected returns.
  \item Subjective investor views are blended with the equilibrium
  prior through a Bayesian update.
  \item The optimisation step in Black-Litterman is the same as MVO
  with $\boldsymbol{\mu}_{BL}$ replacing the sample mean.                                                    
\end{itemize}
\end{keypoints*}

% ─────────────────────────────────────────────
\subsection{Black-Litterman and its Bayesian approach}

\indent \indent The Black-Litterman (BL) model was developed by Fischer Black and 
Robert Litterman at Goldman Sachs in the early 1990s \citep{black_asset_1991, black_global_1992} 
to address the instability of Mean-Variance Optimisation.

\indent The BL model addresses two shortcomings of classic mean-variance optimisation:
\begin{enumerate}
  \vspace{-2mm}
  \item What expected returns should be used? \newline
  Rather than relying on historical data, BL uses returns implied by current 
  market-cap weights as the neutral starting point.
  \item How should the views be incorporated? \newline
  BL offers a formal Bayesian framework for blending subjective investor views 
  with this equilibrium prior, weighting each source inversely by its uncertainty.
  \vspace{-2mm}
\end{enumerate}

\noindent \emph{Step 1. Equilibrium Returns} \\
\indent The model takes as its starting point the equilibrium expected returns implied 
by the market portfolio, i.e. the portfolio where each asset is held in proportion
to its market capitalisation.
Assuming the market portfolio is mean-variance efficient, the equilibrium excess returns are:
\begin{equation}
\Pi = \delta \Sigma w_{m}
\end{equation}
where $w_m$ is the market-cap weight vector, $\Sigma$ is the covariance 
matrix of asset returns and $\delta$ is the risk-aversion coefficient
\begin{equation}
  \delta = \frac{r_m - r_f}{\sigma_m^2}
\end{equation}
with $r_m$ the return of the market portfolio and $\sigma^2$ its variance. \\

\noindent \emph{Step 2. Express Views} \\
\indent A view can be absolute or relative, for example "asset A will return 5\%" or 
"asset A will outperform asset B by 2\%".
Each view has two components: the expected return itself and a confidence 
level attached to it. \newline
\indent Formally, k views are expressed through the following system:
\begin{equation}
  P \boldsymbol{\mu} = \boldsymbol{Q} + \boldsymbol{\varepsilon}
  \qquad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \Omega)
\end{equation}
where
\begin{itemize}
  \item $P \in \mathbb{R}^{k \times n}$: the \emph{pick matrix} (which assets are involved)
  \item $\boldsymbol{\mu} \in \mathbb{R}^k$ is the expected return
  \item $\boldsymbol{Q} \in \mathbb{R}^k$: the expected view returns
  \item $\Omega \in \mathbb{R}^{k \times k}$: view uncertainty (diagonal)
  \item $\boldsymbol{\varepsilon}$ (error-term): the deviation of true returns from the stated views, 
  assumed to be normally distributed with mean zero
\end{itemize}

\indent Assets are ranked by their return over the previous 12 months, excluding the most 
recent month to eliminate short-term reversal effects. The highest-return tercile forms
the winner portfolio, while the lowest-return decile constitutes the loser portfolio. \\
\indent The view is expressed as:
\begin{equation}
    \boldsymbol{p}_{k}^{\top} \boldsymbol{\mu} = q_k
\end{equation}
$\boldsymbol{p}_k$ assigns equal positive weights to winner assets and equal negative weights 
to loser assets, forming a long-short momentum view. The expected return 
spread $q_k$ is estimated over a rolling window at each rebalancing date as asset rankings change.

\noindent \emph{Step 3. Bayesian Update} \\
\indent The prior on expected returns is:
\begin{equation}
    \boldsymbol{\mu} \sim \mathcal{N}(\boldsymbol{\Pi}, \tau \boldsymbol{\Sigma})
\end{equation}
This encodes the belief that expected returns are centred on the equilibrium 
vector $\boldsymbol{\Pi}$, with uncertainty scaled by $\tau$. Combining this prior 
with the view system via Bayes' theorem yields the posterior distribution over 
expected returns:
\begin{equation}
    \boldsymbol{\mu} \mid \boldsymbol{Q} \sim \mathcal{N}(\boldsymbol{\mu_{BL}}, \boldsymbol{\Sigma_{BL}})
\end{equation}
where the posterior mean is:
\begin{equation}
    \boldsymbol{\mu_{BL}} = 
    \left[(\tau \Sigma)^{-1} 
    + P^{\top}\Omega^{-1}P\right]^{-1}\left[(\tau \Sigma)^{-1}\boldsymbol{\Pi} 
    + P^{\top}\Omega^{-1}\boldsymbol{Q}\right]
\end{equation}
The posterior mean $\boldsymbol{\mu}_{BL}$ is a precision-weighted average of the 
equilibrium prior $\boldsymbol{\Pi}$ and the investor's views $\boldsymbol{Q}$, with each 
source weighted inversely by its uncertainty. 
When views are diffuse (large $\boldsymbol{\Omega}$), $\boldsymbol{\mu}_{BL}$ 
remains close to $\boldsymbol{\Pi}$. 
If the views are held with high confidence (small $\boldsymbol{\Omega}$), 
$\boldsymbol{\mu}_{BL}$ tilts toward $\mathbf{Q}$. \newline

This blending property is central to the appeal of the BL framework: 
Rather than overriding the equilibrium prior, views are incorporated gradually 
and in proportion to their confidence. This acts as a natural regulariser, because the 
prior pulls weights toward the market portfolio, the model is far less prone to the 
concentrated, unstable allocations that plague classical MVO. \\

\noindent \emph{Step 4. Portfolio Optimisation} \\
\indent The optimal portfolio is obtained by applying the same Sharpe-maximising 
objective derived in Section~\ref{sec:mvo} (equation~\ref{eq:msr}), 
substituting $\boldsymbol{\mu}_{\text{BL}}$ in place of the sample mean $\boldsymbol{\mu}$.


\subsection{Dalio's Risk Parity}


\section{Methodology}

\subsection{Data}
The dataset contains daily adjusted closing prices for an 18-asset universe over a 10-year period, spanning from January 1st 2015, to December 31st 2025. The universe is made to reflect a representative UK-centric multi-asset portfolio. It consists of 15 large-cap equities across various sectors, which include Financials, Energy, and healthcare, as well as three diversifying exchange-traded products: UK Government Gilts (IGLT.L), Gold (SGLD.L), and an extensive Commodities basket (WCOG.L). All of this pricing data is programmatically sourced via Yahoo Finance.

Standard closing prices were not chosen, instead adjusted closing prices were explicitly used because they account for corporate actions such as stock splits, rights issues, and dividend distributions. This is a crucial prerequisite for calculating precise, time-additive daily log returns. Moreover, the use of raw prices would distort the return metrics during dividend ex-dates, so adjusted prices are used since they accurately reflect the true total return an investor would capture over a decade-long horizon encompassing multiple macroeconomic regimes.

\subsection{Model Implementation}
While Section~2 establishes the theoretical framework of portfolio optimisation, this section details the practical parameters imposed to translate these theories into feasible algorithmic strategies. Rather than relying on generic, pre-packaged optimisation libraries, the architecture utilises a custom Object-Oriented Programming paradigm built on \texttt{NumPy} and \texttt{Pandas}. An abstract base class (\texttt{BaseStrategy}) is defined to ensure modularity across models, utilising a rolling 252-trading-day estimation window to dynamically generate covariance ($\Sigma$) and expected return ($\mu$) inputs.

To mitigate the ``error maximisation'' fragility highlighted in the literature review, our solvers mathematically enforce several structural constraints. In the case of Mean-Variance Optimisation (MVO), we derive the maximum Sharpe ratio portfolio via an iterative subspace method. Numerical stability and positive definiteness within highly correlated asset matrices are guaranteed by applying a Tikhonov regularisation penalty (a diagonal loading of $1 \times 10^{-8}$) to the covariance matrix. Concurrently, the algorithm strictly adheres to a long-only constraint ($w_i \geq 0$) by systematically pruning assets with negative target weights from the active matrix, re-solving iteratively until a strictly non-negative global optimum is achieved.

Moving to the Risk Parity framework, the equal risk contribution portfolio requires a distinct numerical approach. Here, a custom Newton--Raphson iteration serves as the root-finding algorithm. By utilising an analytically derived Jacobian matrix initialised with an inverse-volatility guess, this method ensures both rapid and precise convergence. 

The Black--Litterman model likewise requires a systematic methodology to generate its subjective views ($Q$). We construct a cross-sectional 252-day price momentum signal, directly motivated by the momentum effect documented by \citet{jegadeesh_returns_1993}. Because their research demonstrates that stocks with high returns over the prior 3 to 12 months continue to generate abnormal outperformance of approximately $1\%$ per month, this empirically grounded signal provides a robust foundation for our directional views. These historical return signals are subsequently blended with the implied equilibrium returns via Bayesian updating to yield the final posterior expected returns.
% \indent The investor views incorporated in this study are motivated by the momentum effect documented by 
% \citet{jegadeesh_returns_1993}. They show that stocks with high returns over the prior 3-12 
% months continue to outperform stocks with low prior returns over the subsequent 3-12 months, 
% generating abnormal returns of approximately 1\% per month.
% This persistent pattern offers an empirically grounded basis for constructing 
% views within the Black-Litterman framework. 

\subsection{Backtesting Framework}
The strategies are evaluated using a walk-forward backtesting engine to ensure zero look-ahead bias. At any given rebalancing date $t$, the optimiser relies strictly on the historical data available in the preceding 252-day window. A starting capital base of \pounds 100{,}000 is initialised in the portfolio and deployed immediately on the first available trading day. Thereafter, the portfolio undergoes rebalancing on the final trading day of each calendar month.

Critically, the simulation relies on several foundational assumptions. We assume perfect asset divisibility, allowing the allocation of fractional shares to match exact target weights. In addition, we assume immediate execution at the end-of-day adjusted close price on the rebalancing date. To simulate real-world implementation frictions and measure the impact of portfolio turnover, a flat transaction cost of 5 basis points ($0.05\%$) is deducted per unit of traded value during each monthly rebalance. Any capital not allocated to the active asset matrix is held as an uninvested cash balance yielding $0\%$. Furthermore, the simulation does not explicitly model non-linear market impact or bid--ask spread expansion during liquidity crises. It is assumed that the 18 selected large-cap assets possess sufficient market depth to absorb the hypothetical order flow without inducing severe execution slippage.


\subsection{Performance Metrics}
All metrics we're using
(Return, Sharpe, Sortino, Volatility, CAGR, Max DD, VaR, CVaR)

\section{Results}
This section presents the key findings from your analyses and interprets their significance. Use figures and tables to 
present your results clearly.

Present your main results. This could be in the form of tables summarizing statistical outputs, or charts showing trends and relationships.


\subsection{Main Result: Performance Comparison}

Discuss what your results mean. How do they relate to your initial research question? Are they consistent with existing literature? What are the implications of your findings?


\subsection{Risk-Return Trade-off}


\subsection{Regime Dependence}


\subsection{Portfolio Characteristics}


\section{Discussion}
Summarize the main conclusions of your research. Reiterate the key insights and their importance.


\subsection{Why did 1/N perform well?}


\subsection{The risk parity puzzle}


\subsection{Practical Implications}



\subsection{Limitations}


\section{Conclusion}



\section{Declarations}
Briefly describe the contribution of each author to the research and writing of the report. For example: "A.B. designed the research. C.D. collected the data. E.F. performed the analysis. All authors contributed to writing the report."



Future work could include:
\begin{itemize}
    \item Exploring alternative methodologies or datasets.
    \item Addressing limitations of the current study.
    \item Expanding the research to a different market or asset class.
\end{itemize}


\begin{table}[bt!]
\caption{Descriptive caption for your table.}\label{tab:example_table}
\begin{tabular}{l c c c}
\toprule
Category & Metric 1 & Metric 2 & Metric 3 \\ 
\midrule
Group A & 0.00 & 0.00 & 0.00 \\ 
Group B & 0.00 & 0.00 & 0.00 \\ 
Group C & 0.00 & 0.00 & 0.00 \\ 
\bottomrule
\end{tabular}

\begin{tablenotes}
\item Note: Explain any specific details about the data in the table.
\end{tablenotes}
\end{table}

\bibliography{../refs/refs}

\end{document}
